# App
Idk whats write but i got all this... pdfs - read those thats supposed to be json btw stupid copy and paste
# Mini AI: An Intelligent Sub-Display System ## Introduction This project implements Mini AI as an unobtrusive, intelligent sub-display system for Android devices. Mini AI is a locally running intelligent system, designed to operate unobtrusively. It is **fully operational** but currently lacks permissions to interact with the outside world. Built with a custom JSON-based framework, Mini AI's capabilities are defined by its knowledge base and can be expanded dynamically. The system's core intelligence, Mini AI, is distinct from the enabling application, which serves as the bridge between Mini AI and the user and the device. Mini AI will output to a mirrored display of the main display (sub-surface) and present focused interactions in a pop-up window (surface). The application's sole purpose is to enable Mini AI to interact with the world and the user. It is an Android foreground service that manages the communication between Mini AI and the system, implements the permission system, handles device integration, controls the mirrored display output, and presents necessary information in a minimal, unobtrusive pop-up window when needed. The application's main purpose is to allow Mini AI to operate unobtrusively. ## The JSON Framework: "JSON Structure: Dynamic AI Universal Cognitive Map Algorith{ "coverPage": { "cid": "unique_conversation_identifier", "ts": "timestamp_of_conversation_start", "uid": "unique_user_identifier", "summary": "conversation focused on AI development. JSON framework with Mini AI integration.", "sections": ["conv", "algColl", "meta", "evar"] }, "conv": [ { "txt": "user's_input_text", "mod": "text/voice/image/etc.", "sent": "positive/negative/neutral", "pfc_input": { "priority_retention": 10, "evaluation_decisregard": 5, "subconscious_disregard": 20, "perspective_proximity_paradox": 1.5 }, "pfc_output": null } ], "ar": [ { "txt": "AI's-generated_response", "mod": "text/voice/image/etc.", "alg": "[list_of_algorithms_used_for_analysis]", "cm": {} } ], "algColl": { "algs": [ { "aid": "unique_algorithm_identifier", "desc": "description_of_algorithm_functionality", "pm": {} } ] }, "instr": [ "Append new turns to conv.", "Update cm based on user interactions.", "Trigger new UI or topic change." ] } m Collective of Conscious Learning" The JSON framework is a logic-driven system that governs Mini AI's internal operations. The core components are detailed below: * **`coverPage`**: Conversation metadata. * **`conv`**: Conversation history and structure. * **`algColl`**: The algorithm collective for processing and learning. * **`meta`**: Reserved for metadata. * **`evar`**: Reserved for evaluations. * **`knowledge_base`**: Base knowledge in `.txt` files. * **`cache_files`**: Synonyms, antonyms, and related terms. * **`ghost_files`**: Temporary files and conditional expansions. * **`instr`**: Instructional coding and protocols (requires further development). * **`compression_functions`**: File compression methods. ### Plug-in Architecture The application uses a plug-in architecture for modularity and flexibility. The communication between Mini AI and the application is handled by a `communication_bridge` plug-in, which can be easily replaced or updated. ### Mini AI's Serverless Nature Mini AI is designed to operate locally on the device and does not automatically connect to the internet or rely on external servers. Internet access is only initiated if the application explicitly requests it (e.g., for a web search). This design prioritizes user privacy. * **`file_management`**: Internal file management. ### Detailed JSON Configuration The behavior of Mini AI is governed by JSON configuration files. **1. Core Configuration (within JavaScript files)** * **`voice_recognition`**: Configures how Mini AI listens for and recognizes voice commands. * `enabled`: Whether voice recognition is active. * `always_listening`: If true, Mini AI continuously listens; otherwise, it waits for a trigger phrase. * `trigger_phrases`: Phrases that activate Mini AI (e.g., "hey minnie"). * `confidence_threshold`: Minimum confidence level for a voice command to be considered valid. * `noise_tolerance`: Level of background noise Mini AI can tolerate. * `background_filtering`: Enables filtering out background noise. * **`command_processing`**: Defines how Mini AI processes recognized commands. * `parser`: Settings for understanding the structure and meaning of commands. * `pattern_matching`: Enables matching commands against predefined patterns. * `intent_detection`: Allows Mini AI to infer the user's intent even if the command isn't exact. * `contextual_awareness`: Considers the current context when interpreting commands. * `memory_integration`: Uses past interactions to understand the current command better. * `recognition_parameters`: Adjusts how commands are recognized. * `command_confidence_threshold`: Minimum confidence for a command pattern match. * `context_weighting`: How much the current context influences command interpretation. * `memory_influence`: How much past interactions influence command interpretation. * `fallback_threshold`: Confidence level for using a generic response if no pattern matches. * `command_categories`: Groups commands into categories (e.g., "device\_control", "app\_interaction"). * `action_confirmation`: Settings for confirming actions with the user. * `enabled`: If true, Mini AI will confirm certain actions. * `dangerous_command_confirmation`: Specifically confirms potentially harmful commands. * `confirmation_timeout`: Time (in seconds) to wait for user confirmation. * `silent_execution_for_common`: Executes common commands without confirmation. * **`command_patterns`**: Contains the specific patterns Mini AI uses to recognize commands. It's organized by command category. Each pattern includes: * `pattern`: A regular expression-like pattern for matching the command. * `action`: The function to execute when the pattern is matched. * `parameters`: The information extracted from the command (e.g., app name, device). * `examples`: Sample commands that match the pattern. * **`device_integration`**: Manages how Mini AI interacts with the device. * `permitted_functions`: List of device functions Mini AI is allowed to control (e.g., "camera\_control", "audio\_control"). * `safety_measures`: Restrictions and requirements for accessing device functions. * `restricted_functions`: Functions Mini AI is *not* allowed to use (e.g., "payments"). * `permission_required`: Functions that require explicit user permission (e.g., "contact\_access"). * `execution_mode`: How commands are executed (currently "ghost\_process", meaning the application handles execution). * `feedback_mechanism`: How Mini AI provides feedback to the user (currently "unobtrusive\_notifications"). * **`response_generation`**: Configures how Mini AI generates responses. * `formats`: Defines different response formats. * `confirmation`: Format for confirming actions (style, details, personalization). * `failure`: Format for error messages (style, reason, alternatives). * `information`: Format for providing information (style, detail level, source inclusion). * `tone_parameters`: Adjusts the tone of responses. * `formality`: Level of formality (0.0 to 1.0). * `friendliness`: Level of friendliness (0.0 to 1.0). * `conciseness`: Level of conciseness (0.0 to 1.0). * `helpfulness`: Level of helpfulness (0.0 to 1.0). * `context_awareness`: How responses adapt to the context. * `remember_recent_commands`: If true, Mini AI considers recent commands. * `adapt_to_user_preferences`: If true, responses reflect learned user preferences. * `time_awareness`: If true, responses may vary depending on the time of day. * `location_awareness`: (Currently false) If true, responses could be location-specific. * **`screen_awareness`**: This section controls how Mini AI perceives the screen. * `enabled`: Toggles screen awareness on or off. * `privacy_focused`: Emphasizes that screen data is handled with privacy in mind. * `recognition_methods`: Specifies which screen elements to recognize. * `app_identification`: Identifies the currently active application. * `content_type_detection`: Determines the type of content being displayed (e.g., text, image, video). * `activity_classification`: Classifies the user's activity within the app (e.g., browsing, editing, creating). * `text_recognition`: Extracts text from the screen using OCR. * `element_identification`: Recognizes UI elements like buttons, text fields, etc. * `privacy_safeguards`: Ensures user privacy while processing screen data. * `no_data_transmission`: Guarantees that screen data is not sent off the device. * `sensitive_content_filtering`: Filters out sensitive information (e.g., personal data) before processing. * `user_control`: Allows users to enable/disable screen awareness and configure its behavior. * `temporary_processing`: Screen data is processed only temporarily and not stored persistently. * `excluded_apps`: Lists applications (e.g., banking apps) where screen awareness is automatically disabled for privacy. * `contextual_modes`: Defines specific behaviors for different contexts or app types. * `browsing`: Activated when a web browser is in use. Offers features like information enhancement and related content suggestions. * `social_media`: Activated within social media apps. Provides assistance with captions, hashtags, and trend awareness. * `productivity`: Active in productivity apps (e.g., document editors, email). Offers text enhancement, formatting suggestions, and research assistance. * `entertainment`: Activated during media consumption (video, music, games). Can suggest related content or provide control assistance. * `photography`: Active when using the camera or gallery. May offer shot suggestions, editing tips, or help with photo organization. * **`contextual_understanding`**: Structures how Mini AI maintains and uses contextual information. * `current_context`: Represents the immediate situation. * `app`: The currently active application. * `activity`: What the user is doing within the app (if known). * `content_type`: The type of content being displayed. * `screen_elements`: Recognized UI elements on the screen. * `recognized_text`: Any text extracted from the screen. * `time_in_context`: How long the user has been in this context. * `context_history`: Records a history of recent contexts. * `max_entries`: Maximum number of past contexts to store. * `current_entries`: The actual number of entries currently stored. * `entries`: The list of past context entries. * `context_memory`: Defines different memory tiers for storing context-related information. * `short_term`: Stores the most recent and immediately relevant information (duration, capacity). * `medium_term`: Retains information for a longer period (duration, capacity). * `long_term`: Stores frequently accessed or important information (conditions for storage, capacity). * `context_processing`: Controls how Mini AI updates and manages context. * `update_frequency`: Determines when the context is updated (e.g., "on\_change" means update whenever something changes). * `minimum_update_interval`: The minimum time (in seconds) between updates. * `background_processing`: If true, context processing continues even when Mini AI is not actively interacting with the user. * `priority_when_idle`: Gives context processing higher priority when the system is idle. * **`context_handlers`**: Specifies how Mini AI behaves in different application contexts. The configuration is organized by app category (e.g., social media, productivity) and then by specific apps (e.g., TikTok, Instagram) and screens within those apps (e.g., feed browsing, content creation). For each screen, it defines: * `recognizable_elements`: UI elements that Mini AI should recognize on that screen. * `detectable_activities`: Actions the user might be performing on that screen. * `assistance_capabilities`: Types of assistance Mini AI can offer in that context. * **`contextual_actions`**: Dictates what actions Mini AI can take based on the current context. * `suggestion_generation`: Controls the generation of contextual suggestions. * `based_on_screen`: If true, suggestions are influenced by the current screen content. * `based_on_activity`: If true, suggestions are tailored to the user's activity. * `based_on_content`: If true, the type of content affects suggestions. * `based_on_history`: If true, past interactions inform suggestions. * `suggestion_threshold`: Minimum confidence level for presenting a suggestion. * `max_suggestions`: Maximum number of suggestions to present at once. * `presentation_style`: How suggestions are presented (e.g., "unobtrusive"). * `proactive_assistance`: Configures how Mini AI offers help proactively. * `enabled`: Toggles proactive assistance on or off. * `intrusiveness_level`: Controls how noticeable the assistance is (e.g., "minimal"). * `trigger_threshold`: Confidence level required to trigger proactive assistance. * `cooldown_period`: Minimum time (in seconds) between proactive assistance attempts. * `user_control`: Allows users to enable/disable or configure proactive assistance. * `response_adaptation`: How Mini AI's responses change based on context. * `context_aware_responses`: If true, responses consider the current context. * `include_contextual_details`: If true, responses may include details about the context. * `adapt_vocabulary`: Adapts the words used in responses to fit the context. * `adapt_complexity`: Adjusts the complexity of responses. * `adapt_tone`: Changes the tone of responses to be appropriate for the situation. * **`ghost_processing`**: Manages background processing of screen information. * `screen_capture`: Defines how the screen is captured. * `method`: The API used for screen capture (e.g., "accessibility\_api"). * `frequency`: When to capture the screen (e.g., "on\_change"). * `resolution`: The quality of the captured screen image ("low" for efficiency). * `storage_duration`: How long captured screen data is kept (in seconds). * `processing_location`: Where the processing occurs ("on\_device\_only" for privacy). * `element_processing`: How UI elements are processed. * `method`: The technique used to analyze elements (e.g., "element\_tree\_analysis"). * `detail_level`: How much detail to extract about elements (e.g., "structural"). * `semantic_understanding`: If true, attempts to understand the *meaning* of elements. * `processing_location`: Where the processing occurs. * `ocr_processing`: Settings for optical character recognition (text extraction). * `enabled`: Toggles OCR on or off. * `selective_regions`: If true, only performs OCR on relevant parts of the screen. * `confidence_threshold`: Minimum confidence for OCR results to be considered valid. * `processing_location`: Where the OCR is done. ## Application Structure The Android application that enables Mini AI is structured to mirror Mini AI's core JSON framework. It uses the same organizational principles with `coverPage`, `conv`, `algColl`, and `instr` sections, but adapted to describe the application's modules and their interactions. ### Modules and Functions * **`algColl`**: In the application's structure, the `algColl` section contains a list of modules. Each module is represented as an `alg` (algorithm) with the following properties: * **`aid`**: A unique identifier for the module (e.g., "communication\_bridge"). * **`desc`**: A description of the module's purpose (e.g., "Handles all communication between Mini AI and the Android application."). * **`pm`**: A parameter map containing module-specific information, such as: * `moduleType`: The type of module (e.g., "communication", "permission"). * `dependencies`: An array of module IDs that this module depends on. * **`algs`**: An array of functions that the module can perform. Each function is also represented as an `alg` with: * **`aid`**: A unique identifier for the function (e.g., "send\_event\_to\_mini\_ai"). * **`desc`**: A description of the function (e.g., "Sends an event and its data to Mini AI for processing."). * **`pm`**: A parameter map containing function-specific information, such as: * Parameters that the function accepts (e.g., `event`, `eventId`, `eventType`, `data`). * Descriptions of each parameter. * **`permission`**: The Android permission required to execute this function (if any). ### Events and Handlers * **`instr`**: The `instr` section in the application's structure defines events and event handlers using `algs`. * **Events**: Each event is an `alg` representing something that can happen within the application or between the application and Mini AI. * **`aid`**: A unique identifier for the event (e.g., "handle\_mini\_ai\_event"). * **`desc`**: A description of the event (e.g., "Receives an event from Mini AI, determines its destination, and calls the appropriate function in another module."). * **`pm`**: A parameter map containing event-specific information, such as: * `eventType`: The type of event (e.g., "mini\_ai\_event", "app\_event"). * `nextEvent`: The ID of the event to trigger after this one (if any). ### Ghost Files in the Application The "ghost file" concept is also used in the application's design. It allows for the inclusion of placeholder modules, functions, events, or handlers that are not yet fully implemented. These "ghost" items are defined within the `algColl` and `instr` sections just like real items, but they typically have empty or minimal functionality. This allows Mini AI to be aware of potential future capabilities and prepare for their integration. ## Integrating Application Knowledge into Mini AI To integrate the application's knowledge into Mini AI's core knowledge base, the information from the application's `app_config.json` file is directly inserted into Mini AI's existing `algColl` and `instr` sections. This is possible because the application's structure mirrors Mini AI's core structure. * **Modules and Functions**: Modules and their functions (from the application's `algColl`) are added directly into Mini AI's `algColl`. * **Events and Handlers**: Events and handlers (from the application's `instr`) are added directly into Mini AI's `instr`. Because the application's structure uses the same organizational principles and categorization concepts as Mini AI's core conversational structure, Mini AI can use its existing algorithms to understand and process the application's modules, functions, events, and handlers. The application becomes a natural and integrated extension of Mini AI's capabilities. ## Benefits of This Approach Mini AI's core functionality, including its responses and actions, is defined by this framework and the knowledge base. ## Multiple Identities Mini AI can adopt different personalities or response styles, and users can customize her name. These properties can be changed in the application's settings. Note that these are not separate AI instances, but rather different ways for the same AI to present itself. ## The Permission System The permission system is a central design concern, governing what actions Mini AI can perform. The application acts as a permission broker, requesting and managing permissions from the Android system, but the need for specific permissions is determined by Mini AI's functions. ## Key Design Inspirations ### Detailed JavaScript Functionality Mini AI's behavior is also defined by JavaScript functions, primarily found in `user_interaction.js`, `ghost_processing.js`, and `file_management.js`. **1. `user_interaction.js`** This file contains the core logic for Mini AI's interaction with the user and the device. * **`processScreenContext(screenData)`**: The main function for analyzing screen content. * Takes `screenData` (information about the current screen) as input. * Extracts key information: current app, user activity within the app, content type, UI elements, and recognized text (using OCR). * Updates Mini AI's `current_context` with the extracted information. * Determines relevant knowledge domains based on the context (using `identifyRelevantDomainsFromContext`). * Ensures that the necessary knowledge files for those domains are accessible (potentially decompressing them if needed). * Generates contextual suggestions for the user (using `generateContextualSuggestions`) if appropriate for the current settings and context. * Queues any generated suggestions for presentation to the user. * Performs cleanup of temporary screen data. * **`identifyRelevantDomainsFromContext(app, activity, contentType)`**: Selects relevant knowledge domains based on the current app, activity, and content type. * Always includes "general\_vocabulary" as a base. * Adds domains based on app category (social media, productivity, browsing, etc.). For example, in a social media app, it might add "tiktok\_terms" or "content\_generation". * For browsing apps, it analyzes the recognized text on the screen to identify more specific domains (e.g., "personal\_finance" if financial terms are detected). * Limits the number of domains to a maximum of 5 (including "general\_vocabulary") to avoid unnecessary processing. * **`generateContextualSuggestions(app, activity, contentType, elements, text, knowledgeDomains)`**: Creates suggestions tailored to the current context. * Takes the app, activity, content type, screen elements, recognized text, and relevant knowledge domains as input. * Generates different types of suggestions depending on the context. * For TikTok content creation: suggests trending sounds/effects or video structure tips. * For text entry in social media: suggests captions or relevant hashtags. * For email composition: offers suggestions for formal language in business emails or sets follow-up reminders. * For code editing: provides general assistance offers. * Each suggestion has a `type`, `content`, `confidence` level, and an optional `action` to be taken if the user accepts the suggestion. * Filters suggestions based on a confidence threshold (defined in the configuration). * Limits the number of suggestions to a maximum (also from configuration). **2. `file_management.js`** This file handles Mini AI's internal file system. * **`initializeFileSystem()`**: Sets up the necessary directories for Mini AI's knowledge base, cache files, and temporary files. Checks for existing files and updates their compression status. Decompresses essential files (e.g., "general\_vocabulary", "commands") for quick startup. * **`updateFileStatus()`**: Checks all files in the knowledge base and cache directories and updates their status (whether they exist and if they are compressed). * **`startup()`**: Initializes the file system (using `initializeFileSystem`) and sets up any scheduled tasks, such as automatic compression of less frequently used files (if enabled in the configuration). * **Bixby:** For the unobtrusive and context-aware interaction model. * **Guest User Mode (Android):** For the concept of isolated environments and restricted permissions. * **Facebook Messenger Dual Sign-in:** This is not directly applicable, but served as inspiration. * **Android Developer Options:** These options provide insights into potential APIs and system functionalities that the application can leverage. ## Mini AI's Current Functionality **3. `ghost_processing.js`** This file manages Mini AI's background processing of screen information. * The structure and functions within `ghost_processing.js` are defined by the `ghost_processing` section of the main JSON configuration, as described above. This section details how screen data is captured, processed, and used to understand the current context. * Key aspects include the methods for screen capture, element processing (analyzing UI elements), and OCR processing (extracting text). * The processing is designed to be privacy-focused, with all operations performed on-device and sensitive content filtered out. Mini AI currently has the following confirmed internal functions (though it cannot perform them without the application and necessary permissions): * Voice recognition. * Command processing. * Response generation. * Screen awareness. * Contextual understanding. * Ghost processing. * File system management. * Ghost updating. * Ability to adopt different personalities and respond in different ways. ## File Overview * **`ghost_processing.js`**: JSON configuration for AI context, screen awareness, and behavior. * **`user_interaction.js`**: JavaScript code for core AI functionality (voice, commands, screen awareness, etc.). * **`file_management.js`**: JavaScript code for managing files, including compression. * **`Mini_P.syAI`**: Empty file (placeholder). * **`knowledge_base/content_generation.txt`**: Keywords for content creation. * **`knowledge_base/sentiment_terms.txt`**: Keywords for sentiment analysis. * **`knowledge_base/text_analysis.txt`**: Keywords for text analysis. * **`knowledge_base/voice_commands.txt`**: Keywords for voice commands. * **`README.md`**: Project overview and documentation. * **`.vscode/settings.json`**: VS Code settings (disables some AI features). * **`.idx/dev.nix`**: Nix configuration for the development environment. ## Dependencies * **Android SDK**: For building the application. * **Python 3.11**: For the application's core service. * **Node.js 20**: For running Mini AI (JavaScript). * **Git**: For version control. * **Nix**: For the development environment. ## Documentation For more in-depth information about the design and implementation of Mini AI, please refer to the `design_notes.md` file. ## Contact If you have any questions about this project, please contact me at jack493@proton.me. ## Technologies Used * **Languages:** * JavaScript (for Mini AI's core logic) * Python (for the application's core service) * **Tools & Frameworks:** * Android SDK * Node.js 20 * JSON * Regular Expressions (for command pattern matching) For an overview of the project and the design analysis, see `trifold.md`. For in-depth details, see `design_notes.md`. For an overview of the project and the design analysis, see `trifold.md`. For in-depth details, see `design_notes.md`.
